# -*- coding: utf-8 -*-
"""ADL_I.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bch7huyW7JWRcN3xYGEIav5rKWl_HOZw
"""

import os
import io
import tensorflow as tf
from tensorflow.keras import layers
import pandas as pd
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import backend as K
K.clear_session()
from tensorflow.keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout, Flatten, RepeatVector,Multiply, Lambda, Dense
from keras.layers import merge
from tensorflow.keras.layers import Input, Dense, LSTM,concatenate, Activation, GRU, SimpleRNN
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam, SGD
from keras.utils.vis_utils import plot_model
import numpy as np
import tensorflow as tf
from tensorflow import keras
#from tensorflow.keras import layers
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler

# Load the Tab seperated values in the dataset
df = pd.read_table('/content/drive/MyDrive/MedicalImages_TRAIN.tsv', header=None, encoding='latin-1')
df_test = pd.read_table('/content/drive/MyDrive/MedicalImages_TEST.tsv', header=None, encoding='latin-1')

# Fill the empty timesteps with 0.0
df.fillna(0.0, inplace=True)
df_test.fillna(0.0, inplace=True)



#preprocess_train
is_timeseries=True
# remove all columns which are completely empty
df.dropna(axis=1, how='all', inplace=True)
if not is_timeseries:
  data_idx = df.columns[1:]
  min_val = min(df.loc[:, data_idx].min())
  if min_val == 0:
    df.loc[:, data_idx] += 1

df.fillna(0, inplace=True)
if not is_timeseries:
  df[df.columns] = df[df.columns].astype(np.int32)

y_train = df[[0]].values
nb_classes = len(np.unique(y_train))
y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min()) * (nb_classes - 1)

# drop labels column from train set X
df.drop(df.columns[0], axis=1, inplace=True)

X_train = df.values

normalize_timeseries=False
if is_timeseries:
  X_train = X_train[:, np.newaxis, :]
  # scale the values
  if normalize_timeseries:
    normalize_timeseries = int(normalize_timeseries)

    if normalize_timeseries == 2:
      X_train_mean = X_train.mean()
      X_train_std = X_train.std()
      X_train = (X_train - X_train_mean) / (X_train_std + 1e-8)

    else:
      X_train_mean = X_train.mean(axis=-1, keepdims=True)
      X_train_std = X_train.std(axis=-1, keepdims=True)
      X_train = (X_train - X_train_mean) / (X_train_std + 1e-8)

y_train

#preprocess_test
is_timeseries=True
# remove all columns which are completely empty
df_test.dropna(axis=1, how='all', inplace=True)
if not is_timeseries:
  data_idx = df_test.columns[1:]
  min_val = min(df_test.loc[:, data_idx].min())
  if min_val == 0:
    df_test.loc[:, data_idx] += 1

df_test.fillna(0, inplace=True)


y_test = df_test[[0]].values
nb_classes = len(np.unique(y_test))
y_test = (y_test - y_test.min()) / (y_test.max() - y_test.min()) * (nb_classes - 1)

# drop labels column from train set X
df_test.drop(df_test.columns[0], axis=1, inplace=True)

X_test = df_test.values

normalize_timeseries=False
if is_timeseries:
  X_test = X_test[:, np.newaxis, :]
  # scale the values
  if normalize_timeseries:
    normalize_timeseries = int(normalize_timeseries)

    if normalize_timeseries == 2:
      X_test_mean = X_test.mean()
      X_test_std = X_test.std()
      X_test = (X_test - X_test_mean) / (X_test_std + 1e-8)

    else:
      X_test_mean = X_test.mean(axis=-1, keepdims=True)
      X_test_std = X_test.std(axis=-1, keepdims=True)
      X_test = (X_test - X_test_mean) / (X_test_std + 1e-8)

X_test,y_test,X_train,y_train

len(X_train[0][0]), len(X_test)

#antirectifier_unit
class Antirectifier(layers.Layer):
    def __init__(self, initializer="he_normal", **kwargs):
        super(Antirectifier, self).__init__(**kwargs)
        self.initializer = keras.initializers.RandomNormal()

    def build(self, input_shape):
        output_dim = int(input_shape[-1])
        self.kernel = self.add_weight(
            shape=(output_dim * 2, output_dim),
            initializer=self.initializer,
            name="kernel",
            trainable=True,
        )

    def call(self, inputs):
        inputs -= tf.reduce_mean(inputs, axis=-1, keepdims=True)
        pos = tf.nn.relu(inputs)
        neg = tf.nn.relu(-inputs)
        concatenated = tf.concat([pos, neg], axis=-1)
        mixed = tf.matmul(concatenated, self.kernel)
        return mixed

    def get_config(self):
        # Implement get_config to enable serialization. This is optional.
        base_config = super(Antirectifier, self).get_config()
        config = {"initializer": keras.initializers.serialize(self.initializer)}
        return dict(list(base_config.items()) + list(config.items()))

#SVD_layer
class SVD_layer(layers.Layer):
  '''def __init__(self, units=128):
    super(SVD_layer, self).__init__()
    self.units=units'''
  '''def build(self, input_shape):
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(name="kernel",   initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(name="bias",initial_value=b_init(shape=(self.units,), dtype='float32'),trainable=True)'''
  #def call(self, inputs):
  '''def call(self, input):
    #inp1, inp2 = input
    print(len(input))
    inp1=tf.reshape(input[0], [1,128])
    inp2=tf.reshape(input[1], [1,128])
    M=tf.concat([inp1, inp2], axis=0)
    
    print(inp1.shape, inp2.shape)
    print(M.shape)
    print(type(M))
    s,u,v=tf.linalg.svd(M, full_matrices=True)
    #print("sing value",s)
    #print("u matrix",u)
    idx=tf.math.argmax(s)
    op = tf.gather(u, idx, axis=1)
    op=tf.expand_dims(op,axis=0)
    #print(op.shape)
    return op'''
  def call(self, input):
    #inp1, inp2 = input
    #print(len(input))
    
    inp1=tf.reshape(input[0], [1,128])
    inp2=tf.reshape(input[1], [1,128])
    M=tf.transpose(tf.concat([inp1, inp2], axis=0))

    
    #print(inp1.shape, inp2.shape)
    print(M.shape)
    print(type(M))
    A=tf.linalg.svd(M, full_matrices=False)

    #print("sing value",s)
    #print("u matrix",u)
    #idx=tf.math.argmax(A[0])
    op = A[1][:,0]
    op=tf.expand_dims(op,axis=0)
    #op=tf.reshape(op,[128,])
    print(op)
    print(op.shape)
    print(type(op))
    return op

input1 = keras.layers.Input(shape=(16,))
print(input1.shape)
x1 = keras.layers.Dense(8, activation='relu')(input1)
input2 = keras.layers.Input(shape=(16,))
x2 = keras.layers.Dense(8, activation='relu')(input2)
x=SVD_layer([x1,x2])#([x1,x2])

x.call([x1,x2])

def calculate_dataset_metrics(X_train):
    """
    Calculates the dataset metrics used for model building and evaluation.
    Args:
        X_train: The training dataset.
    Returns:
        A tuple of (None, sequence_length). None is for legacy
        purposes.
    """
    is_timeseries = len(X_train.shape) == 3
    if is_timeseries:
        # timeseries dataset
        max_sequence_length = X_train.shape[-1]
        max_nb_words = None
    else:
        # transformed dataset
        max_sequence_length = X_train.shape[-1]
        max_nb_words = np.amax(X_train) + 1

    return max_nb_words, max_sequence_length

#extra_preprocessing
if not is_timeseries:
        X_train = pad_sequences(X_train, maxlen=176, padding='post', truncating='post')
        X_test = pad_sequences(X_test, maxlen=176, padding='post', truncating='post')
classes = np.unique(y_train)
le = LabelEncoder()
y_ind = le.fit_transform(y_train.ravel())
recip_freq = len(y_train) / (len(le.classes_) *
                                 np.bincount(y_ind).astype(np.float64))
class_weight = recip_freq[le.transform(classes)]
print("Class weights : ", class_weight)

y_train = to_categorical(y_train, len(np.unique(y_train)))
y_test = to_categorical(y_test, len(np.unique(y_test)))
if is_timeseries:
        factor = 1. / np.cbrt(2)
else:
        factor = 1. / np.sqrt(2)

y_val=y_test[:300]
X_val=X_test[:300]

class peel_the_layer(layers.Layer):
    '''def __init__(self):    
        ##Nothing special to be done here
        super(peel_the_layer, self).__init__()'''
        
    def build(self, input_shape):
        ##Define the shape of the weights and bias in this layer
        ##This is a 1 unit layer. 
        units=1
        ##last index of the input_shape is the number of dimensions of the prev
        ##RNN layer. last but 1 index is the num of timesteps
        self.w=self.add_weight(name="att_weights", shape=(input_shape[-1], units), initializer="normal") #name property is useful for avoiding RuntimeError: Unable to create link.
        self.b=self.add_weight(name="att_bias", shape=(input_shape[-2], units), initializer="zeros")
        super(peel_the_layer,self).build(input_shape)
        
    def call(self, x):
        ##x is the input tensor..each word that needs to be attended to
        ##Below is the main processing done during training
        ##K is the Keras Backend import
        e = K.tanh(K.dot(x,self.w)+self.b)
        a = K.softmax(e, axis=1)
        output = x*a
        
        ##return the ouputs. 'a' is the set of attention weights
        ##the second variable is the 'attention adjusted o/p state' or context
        return a, K.sum(output, axis=1)





def generate_alstmfcn(NUM_CELLS=128):

    ip = Input(shape=(1, 99))
    
    x = LSTM(NUM_CELLS)(ip)
    #attention
    #e=Dense(1, activation='tanh')(x)
    #e=Flatten()(e)
    #a=Activation('softmax')(e)
    #temp=RepeatVector(128)(a)
    #temp=Permute([2, 1])(temp)
    #p = merge.Multiply()([x, temp])
    #p = Lambda(lambda values: K.sum(values, axis=1))(p)
    x = Dropout(0.8)(x)

    y = Permute((2, 1))(ip)
    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)
    y = BatchNormalization()(y)
    y = Antirectifier()(y)
    

    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)
    y = BatchNormalization()(y)
    y = Antirectifier()(y)

    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)
    y = BatchNormalization()(y)
    y = Antirectifier()(y)

    y = GlobalAveragePooling1D()(y)
    
    ko = tf.keras.layers.maximum([x,y])
    #ko=ko.call([x,y])
    #ko=tf.keras.layers.Reshape((-1,128))(ko)
    out = Dense(10, activation='softmax')(ko)


    model = Model(ip, out)

   

    # add load model code here to fine-tune

    return model



model=generate_alstmfcn()
model.summary()

plot_model(model, show_shapes=True, show_layer_names=True)

reduce_lr = ReduceLROnPlateau(monitor='loss', patience=100, mode='auto',
                                  factor=factor, cooldown=0, min_lr=1e-4, verbose=2)

#new_loss
loss1=tf.keras.losses.Poisson(reduction="auto", name="poisson")
loss2=tf.keras.losses.KLDivergence(reduction="auto", name="kl_divergence")
loss3=tf.keras.losses.Huber(delta=1.0, reduction="auto", name="huber_loss")

opt1 = tf.keras.optimizers.Ftrl(
    learning_rate=0.001, learning_rate_power=-0.5, initial_accumulator_value=0.1,
    l1_regularization_strength=0.0, l2_regularization_strength=0.0,
    name='Ftrl', l2_shrinkage_regularization_strength=0.0, beta=0.0
)
opt2=tf.keras.optimizers.Nadam(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name="Nadam")

class MyThresholdCallback(tf.keras.callbacks.Callback):
    def __init__(self, threshold):
        super(MyThresholdCallback, self).__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs=None): 
        val_acc = logs["val_accuracy"]
        if val_acc >= self.threshold:
            self.model.stop_training = True

my_callback = MyThresholdCallback(threshold=0.81)

model.compile(loss='categorical_crossentropy', optimizer=opt2, metrics='accuracy')

history= model.fit(X_train,y_train, validation_data=(X_val, y_val), batch_size=32,callbacks=[my_callback], epochs=2000)







score=model.evaluate(X_test[300:],y_test[300:], batch_size=32)

from matplotlib import pyplot
# plot loss during training
pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
# plot accuracy during training
pyplot.subplot(212)
pyplot.title('Accuracy')
pyplot.plot(history.history['accuracy'], label='train')
pyplot.plot(history.history['val_accuracy'], label='test')
pyplot.legend()
pyplot.show()

model.save("cce_nadam_D2")

model.save_weights("model_cce_nadam_D2.h5")

import shutil
shutil.make_archive('/content/cce_nadam_D2', 'zip', '/content/cce_nadam_D2')

from google.colab import files
files.download('/content/mse_nadam_D1_best.zip')

pred_y=model.predict(X_test)

pred_y[1][35], y_test[1][35]

for i in range(len(y_test[1])):
  if(y_test[1][i]==1):
    print(i)

modelx=keras.models.load_model('/content/mse_nadam_D1.zip')

